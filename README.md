# ğŸŒ¸ Iris Classification â€” KNN vs SVM

A clean, production-style machine learning project comparing **K-Nearest Neighbors (KNN)** and **Support Vector Machine (SVM)** classifiers on the classic **Iris Flower Dataset**.

This project focuses on **proper ML workflow**, **model comparison**, **robust evaluation**, and **interpretable visualizations** â€” without using notebooks.

---

## ğŸ“Œ Project Overview

The goal is to classify iris flowers into three species:

* **Setosa**
* **Versicolor**
* **Virginica**

using four numerical features:

* Sepal length
* Sepal width
* Petal length
* Petal width

Two fundamentally different algorithms are implemented and compared:

* **KNN** â†’ distance-based, instance-driven learning
* **SVM (RBF Kernel)** â†’ margin-based, generalized learning

---

## ğŸ§  Why KNN vs SVM?

| Aspect            | KNN             | SVM                 |
| ----------------- | --------------- | ------------------- |
| Learning type     | Lazy learner    | Eager learner       |
| Decision logic    | Distance voting | Margin maximization |
| Noise sensitivity | High            | Low                 |
| Boundary shape    | Irregular       | Smooth              |
| Inference speed   | Slow            | Fast                |

This makes them ideal candidates for a **meaningful comparison**.

---

## ğŸ“ Project Structure

```
iris-knn-svm/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ iris.csv
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ load_data.py
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ train_knn.py
â”‚   â”œâ”€â”€ train_svm.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ visualize.py
â”‚
â”œâ”€â”€ knn.png
â”œâ”€â”€ svm.png
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“Š Dataset

* **Source:** Iris dataset (originally by Ronald Fisher)
* **Samples:** 150
* **Features:** 4 numeric
* **Classes:** 3

The dataset is stored locally as `data/iris.csv`.
Class labels are encoded numerically during preprocessing.

---

## âš™ï¸ Preprocessing

* Stratified train-test split (80/20)
* Feature scaling using **StandardScaler**
* Label encoding for class targets

Scaling is critical for SVM since it relies on distance-based calculations.

---

## ğŸ¤– Models Implemented

### ğŸ”¹ K-Nearest Neighbors (KNN)

* `k = 5`
* Euclidean distance
* Sensitive to local data structure

### ğŸ”¹ Support Vector Machine (SVM)

* Kernel: **RBF (Gaussian)**
* `C = 1.0`
* `gamma = scale`
* Learns a smooth maximum-margin boundary

---

## ğŸ“ˆ Evaluation Metrics

Each model is evaluated using:

* Accuracy
* Precision
* Recall
* F1-score
* Confusion Matrix

### âœ… Results Summary

| Model | Accuracy   |
| ----- | ---------- |
| KNN   | **93.33%** |
| SVM   | **96.67%** |

SVM performs better, especially in overlapping class regions.

---

## ğŸ–¼ï¸ Decision Boundary Visualization (PCA)

Since the dataset has 4 dimensions, **PCA is used to project data into 2D** for visualization purposes.

### ğŸ”¸ KNN Decision Boundary

* Irregular regions
* Sensitive to overlapping samples

![KNN Decision Boundary](visualization/knn.png)

---

### ğŸ”¸ SVM Decision Boundary

* Smooth, generalized separation
* Better margin handling

![SVM Decision Boundary](visualization/svm.png)

> **Note:** These plots are PCA projections for interpretability and do not represent the full 4D decision space.

---

## â–¶ï¸ How to Run

### 1ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Run the project

```bash
python main.py
```

This will:

* Train both models
* Print evaluation metrics
* Display decision boundary plots

---

## ğŸ§ª Key Learnings

* SVM generalizes better on overlapping classes due to margin optimization
* KNN is intuitive but sensitive to noise and scaling
* Feature scaling is mandatory for SVM
* PCA is useful for visualizing high-dimensional decision boundaries

---

## ğŸš€ Future Improvements

* Hyperparameter tuning with GridSearchCV
* Cross-validation
* Probability calibration for SVM
* Additional classifiers for comparison

---

## ğŸ“Œ Conclusion

This project demonstrates a **clean ML engineering workflow**, meaningful **algorithm comparison**, and **interpretable visualization** â€” making it suitable for portfolios, interviews, and learning reinforcement.

---

â­ If you found this useful, consider starring the repository.
